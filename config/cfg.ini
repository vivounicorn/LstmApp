[file_path]
; 数据存储路径
dump_dir=/home/zhanglei/Gitlab/LstmApp/data/dump/
; 模型权重存储路径
check_point_file_path=/home/zhanglei/Gitlab/LstmApp/data/models/model-{epoch:02d}.hdf5
; 模型训练日志
model_log_path=/home/zhanglei/Gitlab/LstmApp/logs/model.log
; 数据集处理日志
data_log_path=/home/zhanglei/Gitlab/LstmApp/logs/data.log
; 原始样文本文件
poetry_file_path=/home/zhanglei/Gitlab/LstmApp//data/poetrys.txt

[parameters]
; 验证集及测试集比例(1-这两个比例的和就是训练数据比例)
valid_ratio=0.3
test_ratio=0.3

; 这是嵌入单词的向量空间的大小。它为每个单词定义了这个层的输出向量的大小。例如，它可能是32或100甚至更大，可以视为具体问题的超参数
embedding_output_dim=100

; 这是文本数据中词汇的取值可能数。例如，如果您的数据是整数编码为0-9之间的值，那么词汇的大小就是10个单词，注意，如果使用“word2vec”模式，这个值需要设一个很大的值，比如：10000
vocab_size=5000

; 这是输入序列的长度，就像您为Keras模型的任何输入层所定义的一样，也就是一次输入带有的词汇个数。例如，如果您的所有输入文档都由1000个字组成，那么input_length就是1000
embedding_input_length=6

; 优化算法的学习率
learning_rate=0.001

; 训练数据的batch大小
batch_size=32

; 模型lstm层个数及全连接层个数
lstm_layers_num=2
dense_layers_num=1

; 模型训练迭代最大次数
num_epochs=1000

[word2vec]
; 训练语料地址
corpus_file=/home/zhanglei/Gitlab/LstmApp/data/dump/poetrys_words_vector.dat
; 向量输出地址
vec_out=/home/zhanglei/Gitlab/LstmApp/data/w2v_models/
; window大小
window=5
; 向量维度
size=50
; sg,1 for skip-gram; otherwise CBOW
sg=1
; hs
hs=0
; negative
negative=3
